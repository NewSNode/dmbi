**1. What is hierarchical clustering ? Explain any two techniques for finding distance between the clusters in hierarchical clustering ?**

- Hierarchical methods divides all the data sets into hierarchy or tree structure where their are levels.
- Problem with the hierarchical methods is once the cluster formed you can not undo it.
- This type of clustering is very useful for the data visulization and summarization.


Types of Hierarchical Clustering : Agglomerative or divisive

Agglomerative: 

- An agglomerative hierarchical clustering method uses a bottom-up strategy. 
- It typically starts by letting each object form its own cluster and iteratively merges clusters
  into larger and larger clusters, until all the objects are in a single cluster or certain termination conditions are 	 satisfied
- The single cluster becomes the hierarchy’s root
- For the merging step, it finds the two clusters that are closest to each other
- Two clusters are merged per iteration, an agglomerative method requires at most n iterations.


Divisive :

- Divisive hierarchical clustering method employs a top-down strategy
- It starts by placing all objects in one cluster, which is the hierarchy’s root
- It then divides the root cluster into several smaller subclusters, and recursively partitions those clusters into
smaller ones.
- The partitioning process continues until each cluster at the lowest level is coherent enough—either containing only one object, or the objects within a cluster are sufficiently similar to each other.


![Table 4.1](/Images/Figure_5.1.png)


In both Agglomerative or Divisive hierarchical clustering, a user can specify the desired number of clusters as a termination condition.

![Table 4.1](/Images/Figure_5.2.png)

A tree structure called a dendrogram is commonly used to represent the process of
hierarchical clustering. It shows how objects are grouped together (in an agglomerative
method) or partitioned (in a divisive method) step-by-step.


![Table 4.1](/Images/Figure5.3.png)

Measures for distance between the clusters 




**2. Use any hierarchical clustering algorithm to cluster the following 8 examples into three clusters**

A1 = (2,10)
A2 = (2,5)
A3 = (8,4)
A4 = (5,8)
A5 = (7,5)
A6 = (6,4)
A7 = (1,2)
A8 = (4,9) 

Initial cluster centre A1(2, 10), A4(2,5), A7(1,2)
FIRST ITERATION
	A	B	C
A1	0	3.6	8.06
A2	5	4.24	3.16
A3	8.485	5	7.28
A4	3.605	0	7.211
A5	7.0711	3.606	6.708
A6	7.211	4.123	5.385
A7	8.062	7.211	0
A8	2.236	1.414	7.616

Cluster A (A1)
Cluster B (A4, A3, A5, A6, A8)
Cluster C (A2, A7)

Recalculate cluster centre:  
Cluster A: (2, 10)
Cluster B: (6, 6)
Cluster C: (1.5, 3.5)

SECOND ITERATION
	A	B	C
A1	0	5.65	6.519
A2	5	4.12	1.58
A3	8.48	2.82	6.52
A4	3.6	2.23	5.7
A5	7.07	1.41	5.7
A6	7.2	2	4.52
A7	8.06	6.403	1.58
A8	2.23	3.61	6.04

Cluster A (A1, A8)
Cluster B (A3, A4, A5, A6)
Cluster C (A2, A7)

Recalculate cluster centre:  

Cluster A: (3, 9.5)
Cluster B: (6.5, 5.25)
Cluster C: (1.5, 3.5)


THIRD ITERATION
	A	B	C
A1	1.12	6.54	6.52
A2	4.61	4.51	1.58
A3	7.43	1.95	6.52
A4	2.5	3.13	5.7
A5	6.02	0.56	5.7
A6	6.26	1.35	4.53
A7	7.76	6.38	1.58
A8	1.12	4.51	6.04

Cluster A (A1, A8)
Cluster B (A3, A4, A5, A6)
Cluster C (A2, A7)
As we can see there is no changes in the cluster this is the final iteration

**3. Clearly explain the DBSCAN algorithm using appropriate algorithms.**

DBSCAN is a density based clustering algorithm, where the number of clusters are decided depending on the data provided. This is unlike K – Means Clustering, a method for clustering with predefined ‘K’, the number of clusters. Since it is a density based clustering algorithm, some points in the data may not belong to any cluster. Again, this is unlike K – Means Clustering where all the points are assumed to be belonging to some cluster. 

DBSCAN can be explained with the help of it’s two parameters epsilon and min_points being used in the algorithm.

-> Suppose we have a dataset of n-dimensinal data points.
-> For each point in the dataset we make an n-dimensional sphere of radius epsilon around the point and count the number of data points within the sphere.
-> If the number of points within the sphere are more than min_points then we mark the center of the sphere to be belonging to a cluster. We also mark the points inside the sphere to be belonging to the same cluster. We then recursively expand the cluster by applying the same criteria to the points inside the sphere, except the center.
-> Incase the number of points inside the sphere are less than min_points, we ignore it and proceed to the next point in the dataset.

The parameters epsilon and min_points can be determined for the best possible clustering using the dataset itself. This method is called adaptive DBSCAN, which I’m not going to deal with over here. However, for practical purposes we may initialize the values manually if we know the kind of data we will run it on.

The Algorithm

Below is the pseudocode, organized as functions for our purpose. I have picked it up from Wikipedia since it is in sync with what I have explained. The function regionQuery() returns the points within the n-dimensional sphere. The function expandCluster() expands the cluster for each of the points in the sphere.

DBSCAN(D, epsilon, min_points):
      C = 0
      for each unvisited point P in dataset
            mark P as visited
            sphere_points = regionQuery(P, epsilon)
            if sizeof(sphere_points) < min_points
                  ignore P
            else
                  C = next cluster
                  expandCluster(P, sphere_points, C, epsilon, min_points)

expandCluster(P, sphere_points, C, epsilon, min_points):
      add P to cluster C
      for each point P’ in sphere_points
            if P’ is not visited
                  mark P’ as visited
                  sphere_points’ = regionQuery(P’, epsilon)
                  if sizeof(sphere_points’) >= min_points
                        sphere_points = sphere_points joined with sphere_points’
                  if P’ is not yet member of any cluster
                        add P’ to cluster C

regionQuery(P, epsilon):
      return all points within the n-dimensional sphere centered at P with radius epsilon (including P)

Characteristics:
-> DBSCAN is a flexible algorithm, in the sense that it is dynamic with respect to the data.
-> The parameters needed to run the algorithm can be obtained from the data itself, using adaptive DBSCAN.
-> It gives a more intuitive clustering, since it is density based and leaves out points that belong nowhere.
-> It is very fast compared to traditional clustering techniques like K – Means Clustering since it has complexity O(n^2), n being the number of data points.

**4. Explain BIRCH algorithm with example**
ans: Algorithm:
     Phase 1: Load data into memory

Scan DB and load data into memory by building a CF tree. If memory is exhausted rebuild the tree from the leaf node.

Phase 2: Condense data

Resize the data set by building a smaller CF tree

Remove more outliers

Condensing is optional

Phase 3: Global clustering

Use existing clustering algorithm (e.g. KMEANS, HC) on CF entries

Phase 4: Cluster refining

Refining is optional

Fixes the problem with CF trees where same valued data points may be assigned to different leaf entry.
Example:

Clustering feature:

CF= (N, LS, SS)

N: number of data points

LS: $ ∑_{i=1}^N= X_i $

SS:∑Ni=1=X2I
SS:∑i=1N=XI2
enter image description here

(3,4) (2,6)(4,5)(4,7)(3,8)

N=5

NS= (16, 30 ) i.e. 3+2+4+4+3=16 and 4+6+5+7+8=30

$SS=(54,190)=3^2+2^2+4^2+4^2+3^2 =54 \ \ and \ \ 4^2+6^2+5^2+7^2+8^2 =190$

Advantages: Finds a good clustering with a single scan and improves the quality with a few additional scans
Disadvantages: Handles only numeric data
Applications:

Pixel classification in images

Image compression

Works with very large data sets

**5. What is clustering ? Explain k-means clustering alogrithm. Suppose the data for clustering **
   ** { 2, 4, 10, 12, 3, 20, 11, 25 } consider k = 2, cluster the given data using above algorithm.**

    Clustering is a data mining technique used to place data elements into related groups without advance knowledge of the group definitions.
    Clustering is a process of partitioning a set of data in set of meaningful sub-classes, called as clusters.
    A cluster is therefore a collection of objects which are similar between them and are dissimilar to the objects belonging to other clusters.

k-means algorithm:
    K-means clustering is an algorithm to classify or to group the different object based on attributes or features into K number of group.
    K is positive integer number(which can be decided by user)
    Define K centroids for K clusters which are generally far away from each other.
    Then group the elements into clusters which are nearer to the centroid of that cluster.
    After this first step, again calculate the new centroid for each cluster based on the elements of that cluster.
    Follow the same method, and group the elements based on new centroid.
    In every step, the centroid changes and elements move from one cluster to another.
    Do the same process till no element is moving from one cluster to another.

    Algorithm:
    k: number of clusters
    n :sample features vectors $x_1, x_2………x_n$
    $m_i$: the mean of the vectors in cluster i
    Assume k<n< p="">
        Make initial guesses for the mean m_1 , m_2……..,m_k
        Until there is no changes in any mean
            Use the estimated means to classify the samples into clusters.
            For I from 1 to k
        Replace m_i with the mean of all of the samples for cluster i
            End _for
        End _until
        Suppose the data for clustering – 2,4,10,12,3,20,11,25
        Randomly assign means $m_1$=3 and $m_2$=4
        The number which are close to mean $m_1$=3 are grouped into cluster $k_1$ and numbers which are close to mean $m_2$=4 are grouped into cluster $k_2$
        Again calculate the new mean for new cluster groups
        $k_1$={2,3} , $k_2$= {4,10,12,20,30,11,25} , m_1=2.5, $m_2$=16
        $k_1$={2,3,4} , $k_2$= {10,12,20,30,11,25}, $m_1$=3, $m_2$=18
        $k_1$={2,3,4,10}, k_2= {12,20,30,11,25}, $m_1$=4.75, $m_2$=19.6
        $k_1$={2,3,4,10,11,12}, $k_2$= {20,30,25}, $m_1=7, m_2=25$
        $k_1$={2,3,4,10,11,12}, $k_2$= {20,30,25}
        Stop as clusters with these means in step 7 and 8 are same.
        So the final answer is $k_1={2,3,4,10,11,12}, k_2= {20,30,25}$


**6. Use k-means to cluster the following data into 3 clusters.**

Protin and Fat (20,9), (21,9), (15,7), (22,17), (20,8), (25,12), (26,14), (20,9), (18,9), (20,9)

**7. Explain methods of clustering?**

Partitioning methods: Given a databases of n objects or data tuples, a partitioning methods constructs k partitions of data, 
where each partition represents a cluster and k · n. Given k, the number of partitions to construct, it creates an initial 
partitioning. It then uses an iterative relocation technique that attempts to improve the partitioning by moving objects from 
one group to another. The general criterion of a good partitioning is that objects in the same cluster are \close" or related 
to each other, whereas objects of di®erent clusters are \far apart". The k-means algorithm is a commonly used partitioning method.

Hierarchical methods: A hierarchical method creates a hierarchical decomposition of the given set of data objects. It can be 
either agglomerative or divisive. The agglomerative (bottom-up) approach starts with each object forming a separate group. 
It successively merges the objects that are close to one another, until all of the groups are merged into one, or until a termination 
condition holds. The divisive (top-down) approach starts with all of the objects in the same cluster. In each successive iteration, a 
cluster is split up into smaller clusters, until eventually each object forms its own cluster or until a termination condition holds. 
AGNES and DIANA are examples of hierarchical clustering. BIRCH integrates hierarchical clustering with iterative (distance-based) 
relocation.

Density-based methods: These methods are based on the notion of density. The main idea is to continue growing a given cluster as 
long as the density in its \neighborhood" exceeds some threshold. That is, for each data point within a given cluster, the neighborhood 
of a given radius has to contain at least a minimum number of points. This method can be used to ¯lter out noise and discover clusters 
of arbitrary shape. DBSCAN and OPTICS are typical examples of density-based clustering.

Grid-based methods: Such methods quantize the object space into a ¯nite number of cells that form a grid structure. All of the 
clustering operations are performed on the grid structure. The main advantage of this approach is its fast processing time, 
which is typically independent of the number of data objects and dependent only on the number of cells in each dimension in the 
quantized space. STING is an example of grid-based clustering.

Model-based methods: This approach hypothesizes a model for each of the clusters and finds the best ¯t of the data to the given model. 
A model-based algorithm may locate clusters by constructing a density function that re°ects the spatial distribution of the data points. 
It also leads to a way of automatically determining the number of clusters based on standard statistics. It takes \noise" or outliers 
into account, therein contributing to the robustness of the approach. COBWEB and self-organizing feature maps are examples of 
model-based clustering.

Methods for high-dimensional data: High-dimensional data can typically have many irrelevant dimensions. As the dimensionality increases, 
the data usually become increasingly sparse because the data points are likely located in di®erent dimensional subspaces. The distance 
measurement between pairs of points become meaningless and the average density of points anywhere in the data is likely to be low.
Distance- and density-based clustering methods are therefore ine®ective for clustering high- dimensional data. Alternative approaches 
have been proposed, such as subspace clustering methods, which search for clusters in subspaces (or subsets of dimensions) of the data, 
rather than over the entire data space. CLIQUE and PROCLUS are examples of subspace clustering methods. Frequent pattern-based 
clustering is another clustering methodology, which extracts distinct frequent patterns among subsets of dimensions that occur 
frequently. pCluster is an example of frequent pattern-based clustering that groups objects based on their pattern similarity.

Constraint-based methods: These perform clustering by incorporating user-speci¯ed or application oriented constraints. A constraint 
can express a user's expectation or describe \properties" of the desired clustering results, and provides an e®ective means for 
communicating with the clustering process. Constraint-based methods are used in spatial clustering for clustering with obstacle 
objects (e.g., considering obstacles such as rivers and highways when planning the placement of automated banking machines) and 
user-constrained cluster analysis (e.g, considering speci¯c constraints regarding customer groups when determining the best 
location for a new service station, such as ``must serve at least 100 high-value customers"). In addition, semi-supervised 
clustering employs, for example, pairwise constraints (such as pairs of instances labeled as belonging to the same or different 
clusters) in order to improve the quality of the resulting clustering.
 
**7 What are Applicaton of Clustering.
Clustering has wide applications 
 
 - Pattern Recognition
 -Spatial Data Analysis
 - Image Processing
- Economic Science 


